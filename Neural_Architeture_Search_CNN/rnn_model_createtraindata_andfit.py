from SampleSpaceGenerate_CreateandTrainModel import Generate_Sample_Space_and_Create_and_train_model
from keras import optimizers
from keras.layers import Dense, LSTM
from keras.models import Model
from keras.layers import BatchNormalization
from keras.engine.input_layer import Input
from keras_preprocessing.sequence import pad_sequences
import os
import numpy as np

target_classes=2
architecture_len=4

controller_model={
"lstm_dim" :100,
"optimizer": 'sgd',
"learning_rate" : 0.001,
"decay_rate": 0.01,
"momentum" : 0.02,

}


#a class to create the rnn model for training the nas model
class RNN_model_createtraindata_andfit(Generate_Sample_Space_and_Create_and_train_model):

    def __init__(self):
        super().__init__()
        self.rnnmodel_optimizer = controller_model["optimizer"]
        self.rnnmodel_lr = controller_model["learning_rate"]
        self.rnnmodel_decay = controller_model["decay_rate"]
        self.rnnmodel_momentum = controller_model["momentum"]
        self.max_len = architecture_len
        self.rnnmodel_predictclasses = len(self.id_layerconf) + 1
        self.rnnmodel_lstmdim = controller_model["lstm_dim"]
        self.rnnmodel_weights = 'rnnmodel_weights.h5'
        self.all_samples_generated_sofar = []

    #to genrate basemodle sample sequence
    def basemodel_seq_generation(self, model, number_of_basemodel_samples):
        """
            Sample architectures are generated by controller here.
            input params :
            ->  model : rnn model
            ->  number_of_samples : number of sample to generate in each controller sampling epoch

            return params :
            ->  samples
                    type        : list.
                    description  :list of architectures generated.
        """
        #map to store the layer id and type of layer
        con_or_neu={1: 'conv', 2: 'conv', 3: 'conv', 4: 'conv', 5: 'conv', 6: 'conv', 7: 'conv', 8: 'conv', 9: 'conv', 10: 'neural', 11: 'neural', 12: 'neural', 13: 'neural', 14: 'neural', 15: 'neural', 16:'neural', 17: 'neural', 18: 'neural', 19: 'droppout', 20: 'neural'}
        final_layer_id = len(self.id_layerconf)
        dropout_layer_id = final_layer_id - 1
        LAYER__ID = [0] + list(self.id_layerconf.keys())
        generated_samples = []
        print("BASE MODEL ARCHITETURE SAMPLING")
        while len(generated_samples) < number_of_basemodel_samples:
            cur_seq = []
            while len(cur_seq) < self.max_len:
                current_sequence = pad_sequences([cur_seq], maxlen=self.max_len - 1, padding='post')
                current_sequence = current_sequence.reshape(1, 1, self.max_len - 1)
                (probababity_of_each_layer, accuracy_prediction) = model.predict(current_sequence,verbose=0)
                probababity_of_each_layer = probababity_of_each_layer[0][0]
                nex_layer_incur_seq = np.random.choice(LAYER__ID, size=1, p=probababity_of_each_layer)[0]
                #all the if conditions are restrictions for model genration
                if nex_layer_incur_seq==0:
                    continue
                if nex_layer_incur_seq == dropout_layer_id and len(cur_seq) == 0:
                    continue
                if con_or_neu[nex_layer_incur_seq] == "neural" and len(cur_seq) == 0:
                    continue
                if nex_layer_incur_seq == final_layer_id:
                    continue
                if len(cur_seq) == self.max_len-1 :
                    cur_seq.append(final_layer_id)
                    break
                if len(cur_seq) < 2and (con_or_neu[nex_layer_incur_seq] == "conv" or con_or_neu[nex_layer_incur_seq] == "droppout"):
                    cur_seq.append(nex_layer_incur_seq)
                elif len(cur_seq) >= 2 and con_or_neu[nex_layer_incur_seq] == "neural":
                    cur_seq.append(nex_layer_incur_seq)
                else:
                    continue
            if cur_seq not in self.all_samples_generated_sofar:
                generated_samples.append(cur_seq)
                self.all_samples_generated_sofar.append(cur_seq)
        print(generated_samples)
        return generated_samples
    #rnn model which is used for trainig the nas
    def RNNModel_genration(self, rnnmodel_input_shape):
        prev_layer = Input(shape=rnnmodel_input_shape, name='prev_layer')
        "---------------------------------------------------------------------------------------------------"
        tem_layer = LSTM(self.rnnmodel_lstmdim, return_sequences=True)(prev_layer)
        tem_layer = BatchNormalization()(tem_layer)
        basemodel_layer_prediction = Dense(self.rnnmodel_predictclasses, activation='softmax', name='basemodel_layer_prediction')(tem_layer)

        "---------------------------------------------------------------------------------------------------"
        basemodel_valacc = Dense(1, activation='sigmoid', name='basemodel_valacc')(tem_layer)
        "---------------------------------------------------------------------------------------------------"
        model = Model(inputs=[prev_layer], outputs=[basemodel_layer_prediction, basemodel_valacc])
        return model
    #training of rnn model
    def RNNModel_Train(self, model, x_data, y_data, actual_valacc_of_basemodel, rnn_loss_func, batch_size, epochs):
        if self.rnnmodel_optimizer == 'sgd':
            optim = optimizers.SGD(lr=self.rnnmodel_lr, decay=self.rnnmodel_decay,
                                   momentum=self.rnnmodel_momentum, clipnorm=1.0)
        else:
            optim = getattr(optimizers, self.rnnmodel_optimizer)(lr=self.rnnmodel_lr, decay=self.rnnmodel_decay,
                                                                 clipnorm=1.0)
        "---------------------------------------------------------------------------------------------------"
        model.compile(optimizer=optim,
                      loss={'basemodel_layer_prediction': rnn_loss_func, 'basemodel_valacc': 'mse'},
                      loss_weights={'basemodel_layer_prediction': 1, 'basemodel_valacc': 1})
        "---------------------------------------------------------------------------------------------------"
        if os.path.exists(self.rnnmodel_weights):
            model.load_weights(self.rnnmodel_weights)
        "---------------------------------------------------------------------------------------------------"
        print("TRAINING CONTROLLER...")
        model.fit({'prev_layer': x_data},
                  {'basemodel_layer_prediction': y_data.reshape(len(y_data), 1, self.rnnmodel_predictclasses),
                   'basemodel_valacc': np.array(actual_valacc_of_basemodel).reshape(len(actual_valacc_of_basemodel), 1, 1)},
                  epochs=epochs,batch_size=batch_size,verbose=1)
        model.save_weights(self.rnnmodel_weights)
    #predicitng accuracies of generated base model samples for creating trainig data for rnn
    def basemodel_valacc_prediction_from_rnn(self, model, seqences):
        predicted_val_acc = []
        for i in seqences:
            basemodel_sequences = pad_sequences([i], maxlen=self.max_len, padding='post')
            x__ = basemodel_sequences[:, :-1].reshape(len(basemodel_sequences), 1, self.max_len - 1)
            (baselayer_prediction, pred_val_acc) = [x[0][0] for x in model.predict(x__)]
            predicted_val_acc.append(pred_val_acc[0])
        return predicted_val_acc
